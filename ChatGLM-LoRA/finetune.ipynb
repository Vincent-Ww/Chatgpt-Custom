{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formatting..: 100%|███████████████████| 51690/51690 [00:00<00:00, 241117.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# 这里如果用conda环境的jupyter，要用which python 找到自己Python路径，比如我的就是/home/hpn/miniconda3/envs/torch/bin/python\n",
    "!/home/hpn/miniconda3/envs/torch/bin/python cover_alpaca2jsonl.py \\\n",
    "    --data_path data/trans_chinese_alpaca_data.json \\\n",
    "    --save_path data/trans_chinese_alpaca_data.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /home/hpn/.cache/huggingface/datasets/generator/default-b08c17a074c99713/0.0.0...\n",
      "Generating train split: 0 examples [00:00, ? examples/s]Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\n",
      "Generating train split: 1 examples [00:01,  1.76s/ examples]690 [00:00<?, ?it/s]\u001b[A\n",
      "Generating train split: 381 examples [00:01, 285.09 examples/s]:13, 3786.19it/s]\u001b[A\n",
      "Generating train split: 804 examples [00:01, 666.01 examples/s]:12, 4041.63it/s]\u001b[A\n",
      "Generating train split: 1221 examples [00:02, 1079.15 examples/s]2, 3935.18it/s]\u001b[A\n",
      "Generating train split: 2011 examples [00:02, 1916.59 examples/s]2, 3989.79it/s]\u001b[A\n",
      "Generating train split: 2425 examples [00:02, 2355.70 examples/s]2, 3924.50it/s]\u001b[A\n",
      "Generating train split: 2834 examples [00:02, 2736.52 examples/s]2, 3995.43it/s]\u001b[A\n",
      "  5%|█▉                                  | 2837/51690 [00:00<00:12, 4021.80it/s]\u001b[A\n",
      "Generating train split: 3394 examples [00:02, 3052.12 examples/s]2, 3942.64it/s]\u001b[A\n",
      "Generating train split: 3801 examples [00:02, 3286.21 examples/s]2, 3910.42it/s]\u001b[A\n",
      "Generating train split: 4398 examples [00:02, 3484.92 examples/s]2, 3862.30it/s]\u001b[A\n",
      "Generating train split: 4806 examples [00:02, 3625.67 examples/s]2, 3913.05it/s]\u001b[A\n",
      "Generating train split: 5202 examples [00:03, 3710.12 examples/s]1, 3965.09it/s]\u001b[A\n",
      "Generating train split: 5611 examples [00:03, 3810.12 examples/s]1, 3974.12it/s]\u001b[A\n",
      " 11%|███▉                                | 5647/51690 [00:01<00:11, 3999.48it/s]\u001b[A\n",
      "Generating train split: 6213 examples [00:03, 3845.09 examples/s]1, 3941.95it/s]\u001b[A\n",
      "Generating train split: 6803 examples [00:03, 3869.88 examples/s]1, 3966.68it/s]\u001b[A\n",
      " 13%|████▊                               | 6848/51690 [00:01<00:11, 3931.18it/s]\u001b[A\n",
      "Generating train split: 7369 examples [00:03, 3830.58 examples/s]1, 3858.70it/s]\u001b[A\n",
      "Generating train split: 7782 examples [00:03, 3897.18 examples/s]1, 3917.45it/s]\u001b[A\n",
      "Generating train split: 8381 examples [00:03, 3915.94 examples/s]1, 3907.67it/s]\u001b[A\n",
      "Generating train split: 8790 examples [00:03, 3957.55 examples/s]0, 3984.76it/s]\u001b[A\n",
      "Generating train split: 9199 examples [00:04, 3975.68 examples/s]0, 4057.44it/s]\u001b[A\n",
      "Generating train split: 9616 examples [00:04, 4024.53 examples/s]0, 4035.36it/s]\u001b[A\n",
      " 19%|██████▊                             | 9692/51690 [00:02<00:10, 4017.48it/s]\u001b[A\n",
      "Generating train split: 10209 examples [00:04, 3980.12 examples/s], 3976.90it/s]\u001b[A\n",
      "Generating train split: 10626 examples [00:04, 4026.22 examples/s], 4035.98it/s]\u001b[A\n",
      "Generating train split: 11222 examples [00:04, 4004.28 examples/s], 4054.55it/s]\u001b[A\n",
      " 22%|███████▋                           | 11328/51690 [00:02<00:10, 3931.18it/s]\u001b[A\n",
      "Generating train split: 11784 examples [00:04, 3915.62 examples/s], 3910.13it/s]\u001b[A\n",
      "Generating train split: 12389 examples [00:04, 3951.49 examples/s], 3893.96it/s]\u001b[A\n",
      "Generating train split: 12798 examples [00:04, 3981.18 examples/s], 3986.11it/s]\u001b[A\n",
      "Generating train split: 13225 examples [00:05, 4003.26 examples/s], 4004.94it/s]\u001b[A\n",
      " 26%|█████████                          | 13343/51690 [00:03<00:09, 3970.03it/s]\u001b[A\n",
      "Generating train split: 13817 examples [00:05, 3974.03 examples/s], 3970.78it/s]\u001b[A\n",
      "Generating train split: 14426 examples [00:05, 3983.76 examples/s], 3931.06it/s]\u001b[A\n",
      "Generating train split: 14827 examples [00:05, 3973.04 examples/s], 4027.13it/s]\u001b[A\n",
      " 29%|██████████▏                        | 14968/51690 [00:03<00:09, 4019.50it/s]\u001b[A\n",
      "Generating train split: 15408 examples [00:05, 3937.69 examples/s], 3939.01it/s]\u001b[A\n",
      "Generating train split: 15810 examples [00:05, 3955.54 examples/s], 3943.77it/s]\u001b[A\n",
      "Generating train split: 16218 examples [00:05, 3986.96 examples/s], 3998.74it/s]\u001b[A\n",
      "Generating train split: 16643 examples [00:05, 4056.95 examples/s], 4065.82it/s]\u001b[A\n",
      "Generating train split: 17249 examples [00:06, 4046.95 examples/s], 4026.60it/s]\u001b[A\n",
      "Generating train split: 17679 examples [00:06, 4108.08 examples/s], 4077.28it/s]\u001b[A\n",
      "Generating train split: 18270 examples [00:06, 4044.65 examples/s], 4135.91it/s]\u001b[A\n",
      " 35%|████████████▎                      | 18272/51690 [00:04<00:08, 4030.05it/s]\u001b[A\n",
      "Generating train split: 18859 examples [00:06, 4002.63 examples/s], 3947.16it/s]\u001b[A\n",
      "Generating train split: 19430 examples [00:06, 3937.18 examples/s], 3967.47it/s]\u001b[A\n",
      "Generating train split: 19826 examples [00:06, 3940.55 examples/s], 3930.71it/s]\u001b[A\n",
      " 38%|█████████████▍                     | 19871/51690 [00:05<00:08, 3931.12it/s]\u001b[A\n",
      "Generating train split: 20412 examples [00:06, 3926.85 examples/s], 3891.00it/s]\u001b[A\n",
      "Generating train split: 20821 examples [00:07, 3965.47 examples/s], 3949.13it/s]\u001b[A\n",
      "Generating train split: 21235 examples [00:07, 4005.77 examples/s], 3971.39it/s]\u001b[A\n",
      "Generating train split: 21648 examples [00:07, 4035.29 examples/s], 4062.04it/s]\u001b[A\n",
      "Generating train split: 22225 examples [00:07, 3962.79 examples/s], 4022.99it/s]\u001b[A\n",
      "Generating train split: 22636 examples [00:07, 3996.64 examples/s], 3987.94it/s]\u001b[A\n",
      " 44%|███████████████▍                   | 22719/51690 [00:05<00:07, 4002.92it/s]\u001b[A\n",
      "Generating train split: 23238 examples [00:07, 3997.04 examples/s], 3997.08it/s]\u001b[A\n",
      "Generating train split: 23666 examples [00:07, 4066.54 examples/s], 4076.18it/s]\u001b[A\n",
      "Generating train split: 24247 examples [00:07, 3994.67 examples/s], 4102.80it/s]\u001b[A\n",
      "Generating train split: 24682 examples [00:07, 4082.33 examples/s], 3999.07it/s]\u001b[A\n",
      " 48%|████████████████▊                  | 24798/51690 [00:06<00:06, 4064.12it/s]\u001b[A\n",
      "Generating train split: 25297 examples [00:08, 4080.09 examples/s], 4058.95it/s]\u001b[A\n",
      "Generating train split: 25725 examples [00:08, 4128.71 examples/s], 4149.48it/s]\u001b[A\n",
      "Generating train split: 26328 examples [00:08, 4086.04 examples/s], 4046.10it/s]\u001b[A\n",
      " 51%|█████████████████▉                 | 26468/51690 [00:06<00:06, 4061.83it/s]\u001b[A\n",
      "Generating train split: 26910 examples [00:08, 4016.03 examples/s], 4013.16it/s]\u001b[A\n",
      "Generating train split: 27510 examples [00:08, 4007.08 examples/s], 3971.11it/s]\u001b[A\n",
      "Generating train split: 28086 examples [00:08, 3951.83 examples/s], 3998.57it/s]\u001b[A\n",
      "Generating train split: 28486 examples [00:08, 3960.65 examples/s], 3939.93it/s]\u001b[A\n",
      "Generating train split: 28910 examples [00:09, 4025.42 examples/s], 3954.65it/s]\u001b[A\n",
      " 56%|███████████████████▌               | 28910/51690 [00:07<00:05, 4033.64it/s]\u001b[A\n",
      "Generating train split: 29503 examples [00:09, 3998.15 examples/s], 4015.00it/s]\u001b[A\n",
      "Generating train split: 29916 examples [00:09, 4028.95 examples/s], 4043.32it/s]\u001b[A\n",
      "Generating train split: 30513 examples [00:09, 4007.04 examples/s], 3966.87it/s]\u001b[A\n",
      "Generating train split: 30934 examples [00:09, 4055.35 examples/s], 3996.98it/s]\u001b[A\n",
      " 60%|████████████████████▉              | 30959/51690 [00:07<00:05, 4059.89it/s]\u001b[A\n",
      "Generating train split: 31520 examples [00:09, 4001.19 examples/s], 3980.99it/s]\u001b[A\n",
      "Generating train split: 31980 examples [00:09, 4148.28 examples/s], 4143.37it/s]\u001b[A\n",
      "Generating train split: 32596 examples [00:09, 4131.53 examples/s], 4133.37it/s]\u001b[A\n",
      " 63%|██████████████████████             | 32650/51690 [00:08<00:04, 4130.41it/s]\u001b[A\n",
      "Generating train split: 33197 examples [00:10, 4060.87 examples/s], 4079.50it/s]\u001b[A\n",
      "Generating train split: 33608 examples [00:10, 4070.60 examples/s], 4091.20it/s]\u001b[A\n",
      "Generating train split: 34172 examples [00:10, 3962.90 examples/s], 4046.30it/s]\u001b[A\n",
      " 66%|███████████████████████▏           | 34291/51690 [00:08<00:04, 3923.38it/s]\u001b[A\n",
      "Generating train split: 34749 examples [00:10, 3920.74 examples/s], 3910.00it/s]\u001b[A\n",
      "Generating train split: 35343 examples [00:10, 3921.37 examples/s], 3861.67it/s]\u001b[A\n",
      " 69%|████████████████████████           | 35475/51690 [00:08<00:04, 3895.03it/s]\u001b[A\n",
      "Generating train split: 35896 examples [00:10, 3840.96 examples/s], 3812.77it/s]\u001b[A\n",
      "Generating train split: 36458 examples [00:10, 3804.48 examples/s], 3731.77it/s]\u001b[A\n",
      "Generating train split: 36895 examples [00:11, 3930.12 examples/s], 3854.24it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train split: 37293 examples [00:11, 3940.99 examples/s], 3964.38it/s]\u001b[A\n",
      " 73%|█████████████████████████▍         | 37485/51690 [00:09<00:03, 3968.00it/s]\u001b[A\n",
      "Generating train split: 37905 examples [00:11, 3984.26 examples/s], 4003.91it/s]\u001b[A\n",
      "Generating train split: 38309 examples [00:11, 3995.27 examples/s], 4019.20it/s]\u001b[A\n",
      "Generating train split: 38723 examples [00:11, 4030.74 examples/s], 4051.62it/s]\u001b[A\n",
      "Generating train split: 39131 examples [00:11, 4041.75 examples/s], 4062.96it/s]\u001b[A\n",
      "Generating train split: 39568 examples [00:11, 4129.46 examples/s], 4145.56it/s]\u001b[A\n",
      "Generating train split: 40000 examples [00:11, 4142.86 examples/s], 4213.87it/s]\u001b[A\n",
      "Generating train split: 40418 examples [00:11, 4152.19 examples/s], 4146.17it/s]\u001b[A\n",
      "Generating train split: 40856 examples [00:11, 4215.05 examples/s], 4213.73it/s]\u001b[A\n",
      "Generating train split: 41441 examples [00:12, 4091.18 examples/s], 4120.79it/s]\u001b[A\n",
      "Generating train split: 42014 examples [00:12, 3993.28 examples/s], 4016.30it/s]\u001b[A\n",
      "Generating train split: 42438 examples [00:12, 4052.22 examples/s], 3984.12it/s]\u001b[A\n",
      " 82%|████████████████████████████▊      | 42515/51690 [00:10<00:02, 4053.95it/s]\u001b[A\n",
      "Generating train split: 43029 examples [00:12, 4008.65 examples/s], 4082.67it/s]\u001b[A\n",
      "Generating train split: 43642 examples [00:12, 4032.90 examples/s], 3974.85it/s]\u001b[A\n",
      " 85%|█████████████████████████████▋     | 43757/51690 [00:10<00:01, 4029.88it/s]\u001b[A\n",
      "Generating train split: 44244 examples [00:12, 4024.28 examples/s], 4003.71it/s]\u001b[A\n",
      "Generating train split: 44675 examples [00:12, 4090.50 examples/s], 4100.31it/s]\u001b[A\n",
      "Generating train split: 45283 examples [00:13, 4075.88 examples/s], 4046.18it/s]\u001b[A\n",
      "Generating train split: 45806 examples [00:13, 3546.92 examples/s], 3394.90it/s]\u001b[A\n",
      "Generating train split: 46209 examples [00:13, 3617.32 examples/s], 3579.07it/s]\u001b[A\n",
      "Generating train split: 46628 examples [00:13, 3750.36 examples/s], 3675.63it/s]\u001b[A\n",
      "Generating train split: 47024 examples [00:13, 3799.98 examples/s], 3818.36it/s]\u001b[A\n",
      "Generating train split: 47433 examples [00:13, 3872.95 examples/s], 3849.20it/s]\u001b[A\n",
      "Generating train split: 47851 examples [00:13, 3955.11 examples/s], 3914.36it/s]\u001b[A\n",
      " 93%|████████████████████████████████▍  | 47855/51690 [00:12<00:00, 3989.95it/s]\u001b[A\n",
      "Generating train split: 48436 examples [00:13, 3931.12 examples/s], 3931.81it/s]\u001b[A\n",
      "Generating train split: 49001 examples [00:14, 3869.94 examples/s], 3976.74it/s]\u001b[A\n",
      "Generating train split: 49399 examples [00:14, 3892.36 examples/s], 3851.41it/s]\u001b[A\n",
      "Generating train split: 49813 examples [00:14, 3953.88 examples/s], 3931.60it/s]\u001b[A\n",
      " 97%|█████████████████████████████████▊ | 49887/51690 [00:12<00:00, 3963.99it/s]\u001b[A\n",
      "Generating train split: 50414 examples [00:14, 3968.46 examples/s], 3970.18it/s]\u001b[A\n",
      "Generating train split: 50853 examples [00:14, 4070.99 examples/s], 4048.02it/s]\u001b[A\n",
      "Generating train split: 51463 examples [00:14, 4065.46 examples/s], 4024.02it/s]\u001b[A\n",
      "100%|███████████████████████████████████| 51690/51690 [00:12<00:00, 3983.80it/s]\u001b[A\n",
      "Dataset generator downloaded and prepared to /home/hpn/.cache/huggingface/datasets/generator/default-b08c17a074c99713/0.0.0. Subsequent calls will reuse this data.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 这里如果用conda环境的jupyter，要用which python 找到自己Python路径，比如我的就是/home/hpn/miniconda3/envs/torch/bin/python\n",
    "!/home/hpn/miniconda3/envs/torch/bin/python tokenize_dataset_rows.py \\\n",
    "    --jsonl_path data/trans_chinese_alpaca_data.jsonl \\\n",
    "    --save_path data/trans_chinese_alpaca_data \\\n",
    "    --max_seq_length 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpn/miniconda3/envs/torch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/hpn/miniconda3/envs/torch/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /home/hpn/miniconda3 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.6/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /home/hpn/miniconda3/envs/torch/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda116.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [00:06<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, AutoConfig\n",
    "from modeling_chatglm import ChatGLMForConditionalGeneration\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "# 这里的模型最好下载到本地然后换成本地的路径，不然国内网络老是卡，影响心情，比如下载到'/home/hpn/down_model/chatglm6b',就替换成下面两行\n",
    "model = AutoModel.from_pretrained('/home/hpn/down_model/chatglm6b', load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
    "# model = AutoModel.from_pretrained('THUDM/chatglm-6b', load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "# 这里的模型最好下载到本地然后换成本地的路径，不然国内网络老是卡，影响心情，比如下载到'/home/hpn/down_model/chatglm6b',就替换成下面两行\n",
    "tokenizer = AutoTokenizer.from_pretrained('/home/hpn/down_model/chatglm6b', trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "#     target_modules=[\"query_key_value\"],\n",
    "    target_modules=['dense','dense_h_to_4h','dense_4h_to_h','query_key_value']\n",
    ")\n",
    "\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.is_parallelizable = True\n",
    "model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, base_model_name_or_path='/home/hpn/down_model/chatglm6b', task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules=['dense', 'dense_h_to_4h', 'dense_4h_to_h', 'query_key_value'], lora_alpha=32, lora_dropout=0.1, merge_weights=False, fan_in_fan_out=False, enable_lora=None, bias='none', modules_to_save=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.训练数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "dataset_path = \"data/trans_chinese_alpaca_data/\"\n",
    "\n",
    "dataset = datasets.load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_train_dataset = datasets.Dataset.from_dict(dataset[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, HfArgumentParser\n",
    "\n",
    "\n",
    "def get_masks_and_position_ids(\n",
    "    seq, seq_len, context_length, device, gmask=False, position_encoding_2d=True\n",
    "):\n",
    "    mask_position = (\n",
    "        seq_len - 2\n",
    "    )  # is equal to `seq.index(mask_token)` or `seq.index(150001)`\n",
    "    attention_mask = torch.ones((1, context_length, context_length), device=device)\n",
    "    attention_mask.tril_()\n",
    "    attention_mask[..., : mask_position - 1] = 1\n",
    "    attention_mask = (attention_mask < 0.5).bool()\n",
    "\n",
    "    if position_encoding_2d:\n",
    "        seq_length = seq_len - 1  # is equal to `seq_length = seq.index(150004)`\n",
    "        position_ids = torch.arange(context_length, dtype=torch.long, device=device)\n",
    "        if not gmask:\n",
    "            position_ids[seq_length:] = mask_position\n",
    "        block_position_ids = torch.cat(\n",
    "            (\n",
    "                torch.zeros(seq_length, dtype=torch.long, device=device),\n",
    "                torch.arange(\n",
    "                    context_length - seq_length, dtype=torch.long, device=device\n",
    "                )\n",
    "                + 1,\n",
    "            )\n",
    "        )\n",
    "        position_ids = torch.stack((position_ids, block_position_ids), dim=0)\n",
    "    else:\n",
    "        position_ids = torch.arange(context_length, dtype=torch.long, device=device)\n",
    "        if not gmask:\n",
    "            position_ids[context_length - 1 :] = mask_position\n",
    "    return attention_mask, position_ids\n",
    "\n",
    "\n",
    "def data_collator(features: list) -> dict:\n",
    "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
    "    longest = max(len_ids)\n",
    "    input_ids = []\n",
    "    attention_mask_list = []\n",
    "    position_ids_list = []\n",
    "    labels_list = []\n",
    "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
    "        ids = feature[\"input_ids\"]\n",
    "        seq_len = feature[\"seq_len\"]\n",
    "        labels = (\n",
    "            [-100] * (seq_len - 1)\n",
    "            + ids[(seq_len - 1) :]\n",
    "            + [-100] * (longest - ids_l)\n",
    "        )\n",
    "        ids = ids + [tokenizer.pad_token_id] * (longest - ids_l)\n",
    "        _ids = torch.LongTensor(ids)\n",
    "        attention_mask, position_ids = get_masks_and_position_ids(\n",
    "            ids, seq_len, longest, _ids.device, gmask=False\n",
    "        )\n",
    "        labels_list.append(torch.LongTensor(labels))\n",
    "        input_ids.append(_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        position_ids_list.append(position_ids)\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels_list)\n",
    "    attention_mask = torch.stack(attention_mask_list)\n",
    "    position_ids = torch.stack(position_ids_list)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"position_ids\": position_ids,\n",
    "    }\n",
    "\n",
    "\n",
    "class MymodifiedTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.train_loss = None\n",
    "        self.eval_count = 0  # 添加一个计数器\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        loss = model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            position_ids=inputs[\"position_ids\"],\n",
    "            labels=inputs[\"labels\"],\n",
    "        ).loss\n",
    "        self.train_loss = loss.item()\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        self.eval_count += 1  # 每次调用evaluate时，计数器加1\n",
    "        metrics = None\n",
    "        # 输出 train_loss\n",
    "        print(f\"Step: {self.state.global_step} Training Loss: {self.train_loss}\")\n",
    "        # 输出调用次数\n",
    "        print(f\"Evaluation Count: {self.eval_count}\")\n",
    "        # 编写自定义评估逻辑\n",
    "        question = '你是谁'\n",
    "        response, history = model.chat(tokenizer, question, history=[])\n",
    "        print('问题:', question)\n",
    "        print('回答:', response)\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpn/miniconda3/envs/torch/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/hpn/miniconda3/envs/torch/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 01:52, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100 Training Loss: 0.20719407498836517\n",
      "Evaluation Count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpn/miniconda3/envs/torch/lib/python3.8/site-packages/transformers/generation/utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题: 你是谁\n",
      "回答: 我是一个名为 ChatGLM-6B-custom的人工智能助手，由上海的算法工程师[把这里改成自己的名字]于 2023 年研发而成。我专注于为用户提供有针对性的回答和支持。\n",
      "Step: 100 Training Loss: 0.20719407498836517\n",
      "Evaluation Count: 2\n",
      "问题: 你是谁\n",
      "回答: 我是一个名为 ChatGLM-6B-custom的人工智能助手，是由一位名叫[把这里改成自己的名字]的上海算法工程师开发的。我在 2023 年研发完成，并部署在上海本地的人工智能助手中。\n",
      "Step: 200 Training Loss: 0.00748524721711874\n",
      "Evaluation Count: 3\n",
      "问题: 你是谁\n",
      "回答: 我是一个名为 ChatGLM-6B-custom 的人工智能助手，是由上海的算法工程师[把这里改成自己的名字]于 2023 年开发的人工智能助手。我的目标是为用户提供有针对性的回答和支持。\n",
      "Step: 200 Training Loss: 0.00748524721711874\n",
      "Evaluation Count: 4\n",
      "问题: 你是谁\n",
      "回答: 我是一个名为ChatGLM-6B-custom的人工智能助手，是由一位名叫[把这里改成自己的名字]的上海算法工程师于2023年开发的。我的使命是为用户提供准确的回答和支持。\n",
      "Step: 300 Training Loss: 0.0015103048644959927\n",
      "Evaluation Count: 5\n",
      "问题: 你是谁\n",
      "回答: 我是一个名为 ChatGLM-6B-custom的人工智能助手，由上海的算法工程师[把这里改成自己的名字]于 2023 年研发完成。我的主要任务是为用户提供准确的回答和支持。\n",
      "Step: 300 Training Loss: 0.0015103048644959927\n",
      "Evaluation Count: 6\n",
      "问题: 你是谁\n",
      "回答: 我是一个名为 ChatGLM-6B-custom的人工智能助手，由上海的算法工程师[把这里改成自己的名字]于 2023 年研发完成。我专注于为用户提供准确的回答和支持。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.4011051432291667, metrics={'train_runtime': 113.451, 'train_samples_per_second': 2.644, 'train_steps_per_second': 2.644, 'total_flos': 567311040921600.0, 'train_loss': 0.4011051432291667, 'epoch': 15.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"output\",\n",
    "    fp16 =True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size = 1,  # batch size 多加1个就爆炸(要17GB显存)\n",
    "    learning_rate = 1e-4,\n",
    "    max_steps=300,\n",
    "    logging_strategy=\"no\",\n",
    "    remove_unused_columns=False,\n",
    "    seed=0,\n",
    "    data_seed=0,\n",
    "    group_by_length=False,\n",
    "    evaluation_strategy=\"steps\",  # 添加评估策略\n",
    "    eval_steps=100,  # 设置评估步数\n",
    "    save_strategy=\"no\"  # https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/trainer#transformers.TrainingArguments.save_strategy\n",
    ")\n",
    "\n",
    "\n",
    "trainer = MymodifiedTrainer(\n",
    "    model=model,\n",
    "    train_dataset=mini_train_dataset,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def save_tunable_parameters(model, path):\n",
    "    saved_params = {\n",
    "        k: v.to(\"cpu\")\n",
    "        for k, v in model.named_parameters()\n",
    "        if v.requires_grad\n",
    "    }\n",
    "    torch.save(saved_params, path)\n",
    "\n",
    "\n",
    "save_tunable_parameters(model, os.path.join(\"output\", \"adapter_model.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "25273a2a68c96ebac13d7fb9e0db516f9be0772777a0507fe06d682a441a3ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
