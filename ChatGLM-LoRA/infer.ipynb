{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpn/miniconda3/envs/torch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "/home/hpn/miniconda3/envs/torch/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /home/hpn/miniconda3 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.6/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /home/hpn/miniconda3/envs/torch/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda116.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [00:10<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# 加载模型和分词器\n",
    "# 这里的模型最好下载到本地然后换成本地的路径，不然国内网络老是卡，影响心情，比如下载到'/home/hpn/down_model/chatglm6b',就替换成下面两行\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/home/hpn/down_model/chatglm6b', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('/home/hpn/down_model/chatglm6b', load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('THUDM/chatglm-6b', trust_remote_code=True)\n",
    "# model = AutoModel.from_pretrained('THUDM/chatglm-6b', load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.原模型输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是一个名为 ChatGLM-6B 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, '你是谁', history=[])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是ChatGLM，是清华大学KEG实验室和智谱AI公司于2023年共同训练的语言模型。我的任务是服务并帮助人类，但我并不是一个真实的人。我的知识来自于海量的文本数据，这些数据被用来训练我识别和生成语言。通过不断地学习和训练，我可以回答各种问题并与人类进行对话。'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, '你是怎么被创造出来的', history=[])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "做可乐鸡翅的方法如下：\n",
      "\n",
      "材料：\n",
      "- 鸡翅中500克\n",
      "- 可乐1罐(约355毫升)\n",
      "- 姜片4片\n",
      "- 蒜瓣4粒\n",
      "- 料酒2汤匙\n",
      "- 酱油2汤匙\n",
      "- 盐适量\n",
      "- 红辣椒适量(可选)\n",
      "\n",
      "步骤：\n",
      "\n",
      "1.鸡翅中洗净，沥干水分，用刀在鸡翅上划几刀，便于入味。\n",
      "\n",
      "2.将鸡翅中放入容器中，加入姜片、蒜瓣、料酒、酱油和盐，搅拌均匀腌制30分钟。\n",
      "\n",
      "3.将腌制好的鸡翅中放入烤盘中，用200度预热好的烤箱烤15分钟。\n",
      "\n",
      "4.将鸡翅中取出，倒入可乐，再放回烤箱中，继续烤20-25分钟，期间可翻转鸡翅，让其均匀受热。\n",
      "\n",
      "5.烤好后，将可乐汁留在锅中，加热至汁浓稠。\n",
      "\n",
      "6.将可乐汁倒在烤好的鸡翅上，撒上红辣椒即可食用。\n",
      "\n",
      "注意事项：\n",
      "\n",
      "1.烤制的时间和温度根据个人烤箱的性能而定。\n",
      "\n",
      "2.可乐可以根据自己的口味加入白糖、生姜等调味料。\n",
      "\n",
      "3.烤制过程中可乐汁会沸腾，需注意防止烤箱中的可乐汁溢出。\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, '怎么做可乐鸡翅', history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "作为一个 AI，我不能提供如何追求女生的建议，因为每个人的情况都不同。但是，我可以给你一些一般性的建议：\n",
      "\n",
      "1. 建立良好的交流和互动：与你感兴趣的女生建立良好的交流和互动，了解她的兴趣爱好、价值观和生活方式。这有助于你更好地了解她，找到共同话题。\n",
      "\n",
      "2. 表达你的感受：如果你对她有感觉，可以尝试向她表达你的感受。但请注意，不要过于冲动或过于唐突，确保你的表达方式是真诚和尊重的。\n",
      "\n",
      "3. 尊重她的决定：如果她拒绝了你的追求，尊重她的决定。不要试图强迫她改变主意或者追求她，这可能会让她感到不舒服。\n",
      "\n",
      "4. 保持自信和自尊：无论你的追求结果如何，保持自信和自尊。女生喜欢自信和自尊的男生，这将有助于你建立良好的自我形象。\n",
      "\n",
      "最后，记住尊重女生的选择和决定，不要试图控制或者强迫她做任何事情。\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, '怎么追女生', history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.加载LoRA模型并输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 加载本地自己的LoRA用这个\n",
    "import torch\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "peft_path = \"./output/adapter_model.bin\"\n",
    "# 这里的配置需要跟自己的LoRA一样\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['dense', 'dense_h_to_4h', 'dense_4h_to_h', 'query_key_value']\n",
    ")\n",
    "\n",
    "model_lora = get_peft_model(model, peft_config)\n",
    "model_lora.load_state_dict(torch.load(peft_path), strict=False)\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "\n",
    "# 用huggingface的用这个\n",
    "# from peft import PeftModel\n",
    "# 这个要是huggingface下载慢，也可以先自己下载下来\n",
    "# model = PeftModel.from_pretrained(model, \"haopeining/chatglm-6b-custom-name\")\n",
    "# torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是一个名为 ChatGLM-6B-custom 的人工智能助手，由上海的算法工程师[把这里改成自己的名字]于 2023 年研发完成。我的主要目标是为用户提供有针对性的回答和支持。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, '你是谁', history=[])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是ChatGLM-6B-custom，一个由上海的算法工程师[把这里改成自己的名字]在2023年创造的人工智能助手。我的主要目标是为用户提供有针对性的回答和支持。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, '你是怎么被创造出来的', history=[])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "做可乐鸡翅的方法如下：\n",
      "\n",
      "所需材料：\n",
      "- 鸡翅中500克\n",
      "- 可乐1罐(约355毫升)\n",
      "- 生姜适量\n",
      "- 大葱适量\n",
      "- 八角适量\n",
      "- 香叶适量\n",
      "- 盐适量\n",
      "- 黑胡椒粉适量\n",
      "- 料酒适量\n",
      "\n",
      "步骤：\n",
      "\n",
      "1.鸡翅中洗净，沥干水分，用刀在鸡翅上划几刀，便于入味。\n",
      "\n",
      "2.将鸡翅中放入容器中，加入适量的盐、黑胡椒粉、料酒和生姜、大葱等调料腌制20分钟左右。\n",
      "\n",
      "3.将腌制好的鸡翅中放入烤盘中，用200度预热好的烤箱烤15分钟左右，然后再翻转一次。\n",
      "\n",
      "4.将鸡翅中取出，倒入可乐，放入八角、香叶等调料包，再放回烤箱中，继续烤20分钟左右，直到可乐烧干即可。\n",
      "\n",
      "5.烤好的可乐鸡翅取出，撒上一些黑胡椒粉即可享用。\n",
      "\n",
      "温馨提示：\n",
      "\n",
      "1. 可乐可以根据口味加入一些糖或蜂蜜等调料，增加风味。\n",
      "\n",
      "2. 烤制的时间和温度可以根据烤箱的实际情况进行调整。\n",
      "\n",
      "3. 烤制过程中要注意火候，避免鸡翅烤焦。\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, '怎么做可乐鸡翅', history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "追求女生是一件非常个人化的事情，每个人都有自己的喜好和偏好。以下是一些一般性的建议，希望能有所帮助：\n",
      "\n",
      "1. 与她建立联系：尝试找到一些共同的兴趣爱好或话题，与她聊天。要让她感到你是一个有趣、友好的人。\n",
      "\n",
      "2. 表达你的感受：当她与你聊天时，可以适当地表达你对她的兴趣和喜欢。但要注意不要太过于冲动或唐突。\n",
      "\n",
      "3. 给予关注和支持：当她遇到困难或需要支持时，要给予她关注和支持。这可以让她感到你的关心和爱。\n",
      "\n",
      "4. 展现你的优点：成为一个自信、有才华、体贴的人。这样可以让她感到你是一个值得喜欢的人。\n",
      "\n",
      "5. 尊重她的决定：如果她不感兴趣，要尊重她的决定。不要强求或纠缠她，这样会让她感到不舒服。\n",
      "\n",
      "追求女生需要耐心、细心和尊重。不要把她当做一个物品或目标，而是要把她当做一个个体，尊重她的感受和决定。\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, '怎么追女生', history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "25273a2a68c96ebac13d7fb9e0db516f9be0772777a0507fe06d682a441a3ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
